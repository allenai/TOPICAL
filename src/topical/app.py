import os
import random
import urllib.parse
from datetime import datetime
from math import sqrt
from pathlib import Path

import guidance
import nltk
import streamlit as st
import tiktoken
import torch
import ujson as json
from adapters import AutoAdapterModel
from Bio import Entrez
from Bio.Entrez.Parser import DictionaryElement
from guidance import assistant, gen, system, user
from guidance.models import OpenAI
from lxml import html
from more_itertools import chunked
from sentence_transformers import util as sbert_util
from transformers import AutoTokenizer

from topical import nlm, util

DISCLAIMER = (
    "The topic pages in this demo are generated by a Large Language Model (LLM), which can make mistakes."
    " Always verify important information."
)

# Clustering settings
SPECTER_MODEL = "allenai/specter2_aug2023refresh_base"  # base specter model to use for embedding
SPECTER_ADAPTOR = "allenai/specter2_aug2023refresh"  # adaptor to use with chosen specter model
ENCODER_MAX_LEN = 512  # Maximum number of tokens to pass to the encoder
MIN_ARTICLES_TO_CLUSTER = 100  # If there are less than this number of articles, skip clustering
MIN_ARTICLES_TO_AVOID_BACKOFF = 2  # If there are less than this number of clusters, perform backoff
BACKOFF_THRESHOLD = 0.02  # The amount to reduce the cosine similarity by during backoff

# Prompt settings
TOPIC_PAGE_MAX_LEN = 512

# Debug settings
DEBUG_NUM_CLUSTERS = 5  # Number of clusters to display if debug is True
DEBUG_CLUSTER_SIZE = 5  # Size of each cluster to display if debug is True

# Path to the prompts
PROMPT_DIR = Path(__file__).parents[1] / "prompts"

# Setup Entrez API
Entrez.email = os.environ.get("ENTREZ_EMAIL")
Entrez.api_key = os.environ.get("ENTREZ_API_KEY")

random.seed(42)


@st.cache_data(show_spinner=False, max_entries=5)
def get_publications_per_year(records: list[DictionaryElement], end_year: str) -> dict[str, int]:
    """Gets the publications per year (up to an including end_year) in records and returns the counts as a dict."""
    pub_years = [int(nlm.get_year_from_medline_date(record["PubDate"])) for record in records]
    year_counts = {year: pub_years.count(year) for year in sorted(set(pub_years)) if int(year) <= int(end_year)}
    return year_counts


def plot_publications_per_year(year_counts: dict[str, int]) -> None:
    """Plots a histogram of publications per year."""
    st.bar_chart(
        {"Year": list(year_counts.keys()), "Number of Publications": list(year_counts.values())},
        x="Year",
        y="Number of Publications",
        color="#255ed3",
        use_container_width=True,
    )


@st.cache_data(show_spinner=False, max_entries=5)
def preprocess_pubmed_articles(records: DictionaryElement) -> list[dict[str, str | dict[str, str]]]:
    """Performs basic pre-processing of the articles in records and returns them as a list of dictionaries."""
    ptext = "{} / {} articles"
    pbar = st.progress(0, text=ptext.format(0, len(records["PubmedArticle"])))

    articles = []
    for article in records["PubmedArticle"]:
        medline_citation = article["MedlineCitation"]

        pmid = medline_citation["PMID"]
        title = util.sanitize_text(medline_citation["Article"]["ArticleTitle"])

        # Format the abstract
        abstract = medline_citation["Article"].get("Abstract", {})
        abstract = [section.strip() for section in abstract.get("AbstractText", "") if section]
        # Don't include the supplementary material section. Note that this check is not perfect, as
        # sometimes the supplementary material section is not the last section.
        if abstract and abstract[-1].startswith("Supplementary"):
            abstract = abstract[:-1]
        abstract = util.sanitize_text(" ".join(abstract))

        # Get the pubdate
        pubdate = medline_citation["Article"]["Journal"]["JournalIssue"]["PubDate"]
        if pubdate.get("MedlineDate") is not None:
            year = nlm.get_year_from_medline_date(pubdate["MedlineDate"].strip())
            month, day = None, None
        else:
            year = pubdate.get("Year")
            month = pubdate.get("Month")
            day = pubdate.get("Day")

        # remove any html markup, which sometimes exists in abstract text downloaded from PubMed
        if title:
            title = html.fromstring(title).text_content()
        if abstract:
            abstract = html.fromstring(abstract).text_content()

        articles.append(
            {
                "id": pmid,
                "pub_date": {"year": year, "month": month, "day": day},
                "title": title,
                "abstract": abstract,
            }
        )

        # Update the progress bar
        pbar.progress(
            len(articles) / len(records["PubmedArticle"]),
            text=ptext.format(
                len(articles),
                len(records["PubmedArticle"]),
            ),
        )

    return articles


@st.cache_resource(show_spinner=False)
def load_encoder():
    """Load an SPECTER-based text encoder for embedding titles and abstracts."""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = AutoAdapterModel.from_pretrained(SPECTER_MODEL)
    _ = model.load_adapter(SPECTER_ADAPTOR, source="hf", set_active=True)
    model.to(device)
    tokenizer = AutoTokenizer.from_pretrained(SPECTER_MODEL)
    return model, tokenizer


@torch.no_grad()
@st.cache_data(show_spinner=False, max_entries=5)
def embed_evidence(articles: list[str], model: str, _batch_size: int = 32):
    """Jointly embed the titles and abstracts in articles for the given encoder."""
    encoder, tokenizer = load_encoder()

    ptext = "{} / {} articles"
    pbar = st.progress(0, text=ptext.format(0, len(articles)))

    embeddings = []
    for batch in chunked(articles, _batch_size):
        text_batch = [f"{example['title']} {tokenizer.sep_token} {example['abstract']}" for example in batch]
        inputs = tokenizer(
            text_batch,
            padding=True,
            truncation=True,
            return_tensors="pt",
            return_token_type_ids=False,
            max_length=ENCODER_MAX_LEN,
        )
        inputs = inputs.to(encoder.device)
        output = encoder(**inputs)
        embeddings.append(output.last_hidden_state[:, 0, :])
        # Update the progress bar
        curr_num_embeddings = sum(embedding.size(0) for embedding in embeddings)
        pbar.progress(
            curr_num_embeddings / len(articles),
            text=ptext.format(
                curr_num_embeddings,
                len(articles),
            ),
        )
    return torch.cat(embeddings, dim=0)


@st.cache_resource(show_spinner=False)
def load_tiktokenizer(model_choice: str):
    """Load an OpenAI tiktokenizer."""
    return tiktoken.encoding_for_model(model_choice)


@st.cache_data(show_spinner=False, max_entries=5)
def format_evidence(
    articles: list[dict[str, str]], clusters: list[list[int]], model: str, model_max_context: str, prompt_len: int
) -> str:
    """Format the supporting literature as a string for inclusion in the prompt."""
    tokenizer = load_tiktokenizer(model)

    curr_evidence_len = 0
    evidence = [[] for _ in clusters]
    max_length_reached = False

    def _format_article(article: dict[str, str]) -> str | None:
        id_, pub_date, title, abstract = article["id"], article["pub_date"], article["title"], article["abstract"]

        # Take the first 3 and last 2 sentences of the abstract, which we assume are the most informative
        sents = nltk.tokenize.sent_tokenize(abstract)
        if len(sents) > 5:
            abstract = f"{' '.join(sents[:3])} [TRUNCATED] {' '.join(sents[-2:])}"
        else:
            abstract = " ".join(sents)

        date = pub_date["month"].strip() if pub_date["month"] else ""
        date += f" {pub_date['day'].strip()}" if pub_date["day"] else ""
        date += f" {pub_date['year'].strip()}" if pub_date["year"] else ""

        formatted_article = f"PMID: {id_} Publication Date: {date} Title: {title} Abstract: {abstract}"
        formatted_article = util.sanitize_text(formatted_article)
        return formatted_article

    # Compute a weight for each cluster based on the sqrt of its size. If there is remaining space in the prompt
    # after including all centroids, we will sample from the clusters with probability proportional to these weights
    weights = [sqrt(len(cluster)) for cluster in clusters]

    # TODO: Code in this section is heavily duplicated, refactor

    # First, try adding as many centroids as possible
    for cluster_idx in range(len(clusters)):
        article_idx = clusters[cluster_idx].pop(0)
        formatted_article = _format_article(articles[article_idx])
        if not formatted_article:
            continue
        formatted_article_len = len(tokenizer.encode(formatted_article))
        if (curr_evidence_len + formatted_article_len) < (model_max_context - prompt_len - TOPIC_PAGE_MAX_LEN):
            curr_evidence_len += formatted_article_len
            evidence[cluster_idx].append(formatted_article)
        else:
            max_length_reached = True
            break

    # If there is remaining space in the prompt, sample from the clusters with probability proportional to their size
    while not max_length_reached and any(clusters):
        cluster_idx = random.choices(range(len(clusters)), weights=weights, k=1)[0]
        if not clusters[cluster_idx]:
            continue
        article_idx = clusters[cluster_idx].pop(0)
        formatted_article = _format_article(articles[article_idx])
        if not formatted_article:
            continue
        formatted_article_len = len(tokenizer.encode(formatted_article))
        if (curr_evidence_len + formatted_article_len) < (model_max_context - prompt_len - TOPIC_PAGE_MAX_LEN):
            curr_evidence_len += formatted_article_len
            evidence[cluster_idx].append(formatted_article)
        else:
            max_length_reached = True

    evidence = [cluster for cluster in evidence if cluster]

    # TODO: This extra text is not accounted for in the total length, so it's possible to go over the max length
    return "\n\n".join(f"Cluster {i + 1}\n" + "\n".join(cluster) for i, cluster in enumerate(evidence))


@st.cache_resource(show_spinner=False)
def load_llm(model_choice: str, echo: bool = False, max_streaming_tokens: int = TOPIC_PAGE_MAX_LEN) -> OpenAI:
    """Load an OpenAI large language model."""
    return guidance.models.OpenAI(model_choice, echo=echo, max_streaming_tokens=max_streaming_tokens)


@st.cache_resource(show_spinner=False)
def prompt(
    *,
    system_prompt: str,
    instructions_prompt: str,
    how_to_cite_prompt: str,
    total_publications: str,
    canonicalized_entity_name: str,
    publications_per_year: str,
    evidence: str,
    topic_page_prompt: str,
    model: str,
    generate=True,
) -> OpenAI:
    llm = load_llm(model)

    with system():
        response = llm + system_prompt
    with user():
        response += f"# INSTRUCTIONS\n\n{instructions_prompt}\n\n"
        response += f"## HOW TO CITE YOUR CLAIMS\n\n{how_to_cite_prompt}\n\n"
        response += (
            f"# ENTITY OR CONCEPT\n\n"
            f"Canonicalized entity name:{canonicalized_entity_name}\n"
            f"Publications per year: {publications_per_year}\n"
            f"Total number of publications: {total_publications}\n"
            f"Supporting literature:\n\n{evidence}\n\n"
        )
        response += f"# TOPIC PAGE\n\n{topic_page_prompt}\n\n"

    if generate:
        with assistant():
            response += gen("topic_page", temperature=0.0, max_tokens=TOPIC_PAGE_MAX_LEN)

    return response


def main():
    with st.sidebar:
        st.image("https://prior.allenai.org/assets/logos/ai2-logo-header.png", use_column_width=True)

        st.header("Settings")
        st.write("Modifying settings is optional, reasonable defaults are provided.")

        st.subheader("OpenAI API")
        model_choice = st.text_input(
            "Choose a model:",
            value="gpt-4-1106-preview",
            help="Any valid model name for the OpenAI API. It is strongly recommended to use a GPT-4 class model.",
        )
        model_max_context = st.number_input(
            "Maximum input tokens:",
            max_value=16_384,
            value=16_384,
            help=(
                "The maximum number of tokens to allow for the model inputs (minus the prompt plus generated text)."
                " We will pass as many titles and abstracts to the model as possible, given this limit."
                " Roughly speaking, 1 title + abstract consumes about 512 tokens.\n\n"
                " __Note__: You are responsible for ensuring this value is within the limits of the model you choose"
                " (see https://platform.openai.com/docs/models/ for details)."
                " Setting a value larger than a models maximum context length will result in an error."
            ),
        )
        openai_api_key = st.text_input(
            "Enter your API Key:",
            type="password",
            help="Your key for the OpenAI API. Only required if you are running locally.",
        )

        "---"

        st.subheader("Search")
        current_year = datetime.now().year
        end_year = str(
            st.number_input(
                "End year",
                min_value=0000,
                max_value=current_year,
                value=current_year,
                help="Include papers published up to and including this year in the search results.",
            )
        )
        retmax = st.number_input(
            "Maximum papers to consider",
            min_value=1,
            max_value=10_000,
            value=1000,
            step=100,
            help=(
                "Determines the maximum number of papers to consider, starting from most to least relevant.\n\n"
                "__Note:__ Larger values will result in a more comprehensive literature search,"
                " but will take longer to return a topic page."
            ),
        )

        st.subheader("Clustering")
        enable_clustering = st.toggle(
            "Cluster search results",
            value=torch.cuda.is_available(),
            help=(
                "True if search results should be clustered before sampling."
                " Will be automatically disabled if no GPU is available."
            ),
        )
        threshold = st.slider(
            "Cosine similarity threshold",
            min_value=0.92,
            max_value=1.0,
            value=0.96,
            help="Titles and abstracts with a cosine similarity >= this value will be clustered.",
            disabled=not enable_clustering,
        )
        min_community_size = st.slider(
            "Minimum cluster size",
            min_value=1,
            max_value=10,
            value=5,
            help="Clusters smaller than this value will be discarded.",
            disabled=not enable_clustering,
        )

        "---"
        st.subheader("Debug")
        debug = st.toggle("Toggle for debug mode, which displays additional info", value=False)

    st.write("# ü™ÑüìÑ TOPICAL: TOPIC pages AutomagicaLly")
    st.write(
        "TOPICAL produces topic pages for biomedical entities and concepts automatically."
        " Enter a __entity__ or __concept__ of interest below, and a topic page will be generated for you."
        " See [our paper](https://arxiv.org/abs/2405.01796) for more details."
    )
    st.caption('An example is provided, just hit: "__Generate Topic Page__"!')

    with st.expander("Search tips üí°"):
        st.write(
            "TOPICAL supports the full syntax of the PubMed Advanced Search Builder, allowing you to, for example,"
            " search papers that mention a certain __entity__ or __concept__ in the title, e.g. `<entity>[title]`,"
            " or to easily include synonyms, e.g. `<entity> OR <synonym>`. However, in most cases simply entering"
            " the entity verbatim will work well, as PubMed will apply 'automatic term mapping' (ATM) to this query"
            " to include, among other things, matching MeSH descriptors and pluralization."
        )

        st.write(
            "It is generally recommended to use the __Canonicalized name__ field to provide a canonicalized name"
            " for your entity or concept of interest. This will not be used to query PubMed, but it can help keep"
            " the model on track when generating topic pages, especially in cases where the entity has multiple,"
            " potentially ambiguous names. Think of it like a title for your topic page!"
            " If no canonicalized name is provided, the search query will be used instead."
        )

    query = st.text_input(
        "Search query",
        value="long covid",
        help=(
            "Enter a search query for your entity of interest. This supports the full syntax of the [PubMed"
            ' Advanced Search Builder](https://pubmed.ncbi.nlm.nih.gov/advanced/). See "_Search tips_" for more help.'
        ),
    ).strip()

    entity = st.text_input(
        "Canonicalized name",
        value='Post-acute COVID-19 Syndrome (PACS) or "long COVID"',
        help=(
            "Enter a canonicalized name for the entity. This will not be used to query PubMed, but it can help keep"
            " the model on track when generating topic pages, especially in cases where the entity has multiple,"
            " potentially ambiguous names. Defaults to Search query if not provided."
        ),
    ).strip()

    # Default to using the search query
    entity = entity or query

    # Load the API key from an env var, with the UI taking precedence
    openai_api_key = openai_api_key or os.environ.get("OPENAI_API_KEY")

    if not openai_api_key:
        st.warning("Please provide an OpenAI API key in the sidebar.", icon="üîë")
        st.stop()

    # Any key provided in the sidebar will override the environment variable
    os.environ["OPENAI_API_KEY"] = openai_api_key

    if enable_clustering and not torch.cuda.is_available():
        st.warning("Clustering is enabled but no GPU detected. Expect things to be painfully slow", icon="üê¢")

    if st.button("Generate Topic Page", type="primary"):
        with st.status(f"Generating topic page for _{entity}_...", expanded=debug):
            if debug:
                st.info("Debug mode is enabled, additional information will be displayed in blue.", icon="üêõ")

            st.write("Querying PubMed...")
            esearch_results = next(
                nlm.esearch(
                    query,
                    db="pubmed",
                    retmax=10_0000,
                    sort="relevance",
                    mindate="0000",
                    maxdate=end_year,
                    use_cache=True,
                    # Only return papers with abstracts available
                    preprocessor=lambda x: x.strip() + " AND hasabstract[All Fields]",
                )
            )
            pmids = esearch_results["IdList"]
            total_publications = int(esearch_results["Count"])
            pubmed_query_linkout = (
                f'https://pubmed.ncbi.nlm.nih.gov/?term={urllib.parse.quote_plus(esearch_results["QueryTranslation"])}'
            )
            st.success(
                f"Found {len(pmids)} papers using query:"
                f" [`{esearch_results['QueryTranslation']}`]({pubmed_query_linkout})",
                icon="üîé",
            )

            st.write("Fetching publication years...")
            records = nlm.efetch(",".join(pmids), db="pubmed", rettype="docsum", use_cache=True)
            st.success("Done, publications by year plotted below", icon="üìä")
            year_counts = get_publications_per_year(records, end_year=end_year)
            plot_publications_per_year(year_counts)

            if len(pmids) > retmax:
                pmids = pmids[:retmax]

            st.write("Downloading titles and abstracts...")
            records = nlm.efetch(",".join(pmids), db="pubmed", rettype="abstract", retmax=retmax, use_cache=True)
            st.success(f"Downloaded titles and abstracts for {len(records['PubmedArticle'])} papers", icon="‚¨áÔ∏è")

            st.write("Preprocessing titles and abstracts...")
            articles = preprocess_pubmed_articles(records)
            st.success(f"Done {len(articles)} abstracts preprocessed", icon="‚öôÔ∏è")

            # This is a large object, so we delete it to free up memory
            del records

            if not articles:
                st.warning(
                    "No abstracts found for your query (it may not be a valid biomedical entity). Try a different"
                    " query?",
                    icon="üòî",
                )
                st.stop()

            # Default clusters are no clusters
            clusters = None

            if not enable_clustering:
                st.warning("Clustering disabled. Expect faster responses but worse quality results.", icon="‚ö†Ô∏è")
            elif enable_clustering and len(articles) >= MIN_ARTICLES_TO_CLUSTER:
                st.write("Clustering evidence...")
                embeddings = embed_evidence(articles, model=SPECTER_MODEL)
                clusters = sbert_util.community_detection(
                    embeddings, min_community_size=min_community_size, threshold=threshold
                )

                # Try lowering the threshold if no clusters are found
                backoff_threshold = threshold
                while len(clusters) < MIN_ARTICLES_TO_AVOID_BACKOFF and backoff_threshold > 0.9:
                    backoff_threshold -= BACKOFF_THRESHOLD
                    st.warning(
                        f"No clusters found with threshold {threshold},"
                        f" trying a lower threshold ({backoff_threshold:.2f})...",
                        icon="‚ö†Ô∏è",
                    )
                    clusters = sbert_util.community_detection(
                        embeddings, min_community_size=min_community_size, threshold=backoff_threshold
                    )

                if not clusters:
                    st.warning("No clusters found for your query. Randomly sampling articles instead", icon="‚ö†Ô∏è")

                max_cluster_size = len(max(clusters, key=len))
                min_cluster_size = len(min(clusters, key=len))
                avg_cluster_size = sum(len(cluster) for cluster in clusters) / len(clusters)
                st.success(
                    f"Found {len(clusters)} clusters (max size: {max_cluster_size}, min size:"
                    f" {min_cluster_size}, mean size: {avg_cluster_size:.1f}) matching your query",
                    icon="‚úÖ",
                )

                if debug:
                    st.info(
                        f"The first {DEBUG_CLUSTER_SIZE} titles of the first {DEBUG_NUM_CLUSTERS} clusters, useful for"
                        " spot checking the clustering. Clusters are sorted by decreasing size. The first"
                        " element of each cluster is its centroid.\n"
                        + "\n".join(
                            f"\n__Cluster {i + 1}__ (size: {len(cluster)}):\n"
                            + "\n".join(f"- {articles[idx]['title']}" for idx in cluster[:DEBUG_CLUSTER_SIZE])
                            for i, cluster in enumerate(clusters[:DEBUG_NUM_CLUSTERS])
                        )
                    )
            else:
                st.warning(
                    f"Less than {MIN_ARTICLES_TO_CLUSTER} total publications found, skipping clustering...", icon="‚ö†Ô∏è"
                )

            clusters = clusters or [[i] for i in range(len(articles))]

            # Design a prompt to get GPT to generate topic pages
            system_prompt = (PROMPT_DIR / "system.txt").read_text().strip()
            instructions_prompt = (
                (PROMPT_DIR / "instructions.txt")
                .read_text()
                .format(
                    end_year=end_year,
                    min_articles_to_cluster=MIN_ARTICLES_TO_CLUSTER,
                )
            ).strip()
            how_to_cite_prompt = ((PROMPT_DIR / "how_to_cite.txt").read_text()).strip()
            topic_page_prompt = (PROMPT_DIR / "topic_page.txt").read_text().strip()
            publications_per_year = ", ".join(
                f"{year}: {count}" for year, count in year_counts.items() if int(year) <= int(end_year)
            )

            tokenizer = load_tiktokenizer(model_choice)

            # Get length of prompt without evidence (and without generating) so we can format the evidence
            response = prompt(
                system_prompt=system_prompt,
                instructions_prompt=instructions_prompt,
                how_to_cite_prompt=how_to_cite_prompt,
                topic_page_prompt=topic_page_prompt,
                canonicalized_entity_name=entity,
                publications_per_year=publications_per_year,
                total_publications=total_publications,
                evidence="",
                model=model_choice,
                generate=False,
            )

            prompt_len = len(tokenizer.encode(response._current_prompt()))
            evidence = format_evidence(
                articles, clusters, model=model_choice, model_max_context=model_max_context, prompt_len=prompt_len
            )

            # Actually make the request to the API
            with st.spinner("Prompting model..."):
                try:
                    response = prompt(
                        system_prompt=system_prompt,
                        instructions_prompt=instructions_prompt,
                        how_to_cite_prompt=how_to_cite_prompt,
                        topic_page_prompt=topic_page_prompt,
                        canonicalized_entity_name=entity,
                        publications_per_year=publications_per_year,
                        total_publications=total_publications,
                        evidence=evidence,
                        model=model_choice,
                        generate=True,
                    )
                # TODO: Unclear why ConstraintException is being thrown. It didn't happen in guidance <0.1.0.
                except Exception as e:
                    st.error("Failed to generate topic page (see error below), please try again.")
                    st.error(e)

        if debug:
            st.info(f"__Evidence__:\n\n{evidence}")
            st.info(f"__Prompt__:\n\n{response._current_prompt()}")

        if response:
            # Basic post-processing
            topic_page = response["topic_page"].strip()
            # Ensure there are only three sections
            topic_page_sections = topic_page.split("\n\n")
            if len(topic_page_sections) > 3:
                topic_page = "\n\n".join(
                    [topic_page_sections[0].strip(), " ".join(topic_page_sections[1:-1]), topic_page_sections[-1]]
                )

            # Hyperlink all the in-line citations
            topic_page = util.replace_pmids_with_markdown_link(topic_page)

            # Add a header and display the topic page
            st.write(f"### {entity}\n\n{topic_page}")

            # Allow user to download raw markdown formatted topic page
            def prepare_topic_page_for_download() -> str:
                return json.dumps(
                    {
                        "entity": entity,
                        "topic_page": topic_page,
                        "pubmed_query": esearch_results["QueryTranslation"],
                        "generated_by": model_choice,
                        "disclaimer": DISCLAIMER,
                    },
                    ensure_ascii=False,
                    indent=4,
                )

            file_name = util.sanitize_text(entity, lowercase=True).replace(" ", "_").replace("-", "_").replace(",", "")
            st.download_button(
                "Download topic page",
                data=prepare_topic_page_for_download(),
                file_name=f"{file_name}.json",
                help="Download a JSON file containing the markdown formatted topic page and additional metadata.",
            )
        else:
            st.stop()

    "---"

    st.caption(DISCLAIMER)


if __name__ == "__main__":
    main()
